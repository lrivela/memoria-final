\chapter{Diseño e implementación} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% parámetros para configurar el formato del código en los entornos lstlisting
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  %escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  %extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  %frame=single,	                % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=[ANSI]C,                % the language of the code
  %otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
  morecomment=[s]{/*}{*/}
}


En este capítulo se describe cómo se han utilizado las herramientas mencionadas en el capítulo \ref{Chapter2} y sus integraciones. Se presenta la arquitectura de alto nivel completa y luego se describe detalladamente desde el consumo de datos hasta la salida del proceso y su almacenamiento.


\section{Arquitectura del sistema}

En la figura \ref{fig:arqsistema} se puede observar la arquitectura de alto nivel que integra las herramientas utilizadas en la fase de entrenamiento y evaluación de los modelos de IA con las herramientas utilizadas para el despliegue.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{./Figures/arq-sistema.png}
	\caption{Diagrama de la arquitectura de alto nivel.}
	\label{fig:arqsistema}
\end{figure}

La mayor parte del desarrollo se realizó en una computadora local, que contaba con un intérprete de Python y Visual Studio Code con Jupyter para la escritura de código. Estos archivos se sincronizaban contra un repositorio de GitHub.

Los datos para realizar el entrenamiento de los modelos de IA fueron tomados del \textit{data warehouse} BigQuery utilizando consultas SQL y bibliotecas de Google para Python para manejar su ejecución.

Estos datos son llevados a BigQuery por un proceso ajeno a este desarrollo, que se encarga de sincronizar la base de Salesforce hacia el \textit{data warehouse} diariamente. Particularmente, hay una tabla que contiene los reclamos y consultas de los usuarios, con su identificador de caso, fecha de creación y estado.

En las ocasiones en las que fue necesario un mayor poder de cómputo a través del uso de una placa de vídeo dedicada, se utilizó una instancia de Vertex AI Workbench en la nube de Google. Por lo general, estas situaciones se dieron al momento de realizar distintas pruebas con las redes neuronales BERT. Los modelos eran luego exportados al almacenamiento de la nube llamado Cloud Storage en formato ``Pickle'' o ``HDF5'' \textit{(Hierarchical Data Format 5)}.

El repositorio del proyecto cuenta con un Workflow de GitHub Action que se corre manualmente y genera una imagen de Docker, incluyendo bibliotecas y librerías necesarias, archivos de código y de configuración. Esta imagen de Docker es enviada al Artifact Registry de la nube de Google y almacenada allí.

Por otro lado, se trabajó en otro repositorio separado, que es único para manejar todos los flujos de trabajo de Apache Airflow que tiene el área de \textit{machine learning}. Este repositorio es exclusivo para la etapa de despliegue, y se agregó el DAG que administra el proceso de predicción.

El repositorio de Airflow cuenta con un Workflow de GitHub Action que sincroniza el código de los DAGs contra un Cloud Storage que está conectado a la instancia del orquestador. Airflow periódicamente controla si hay un flujo de trabajo nuevo y cuando lo detecta, lo carga.

Al momento de ejecutar el proceso, Airflow utiliza un operador dentro del DAG para conectarse con la plataforma de Kubernetes en instancia un Pod. Este Pod ejecuta los archivos de código del contenedor que se empaquetaron en la imagen de Docker cargada previamente en el Artifact Registry.

Al finalizar, se guardan los resultados de las predicciones en una tabla en BigQuery y el Pod es eliminado de Kubernetes. Los datos quedan disponibles en la base para que cualquier usuario con los suficientes permisos pueda consumirlos ejecutando una consulta a la base.

\section{Extracción y preprocesamiento del texto}

En esta sección se describen en detalle los pasos realizados para obtener los datos crudos y aplicarles un preprocesamiento para poder usarlos de entrada en el posterior entrenamiento de los modelos de IA.

\subsection{Extracción de datos de BigQuery}

Inicialmente se realizó un relevamiento de las tablas existentes, y en particular se encontró una tabla que contenía información de los reclamos y consultas. 

Las columnas de interés de esa tabla fueron:
\begin{itemize}
	\item El identificador del caso.
	\item El número del caso.
	\item La categoría L2 (de segundo nivel).
	\item La categoría L3 (de tercer nivel).
	\item El estado del caso.
	\item La descripción: el contenido del mensaje que escribió el usuario.
	\item La fecha de creación.
	\item La fecha de cierre.
\end{itemize}

Para la fase de entrenamiento, se limitó la extracción de casos a los que tenían fecha de creación entre enero de 2021 y diciembre de 2022. Además, su estado debía ser ``Resuelto'' o ``Cerrado''. Sin embargo, por la poca cantidad de datos que había para algunos clasificadores L2, fue necesario realizar consultas adicionales para tomar casos de 2023. Una vez obtenidos los datos, se guardaron en archivos ``Parquet'' para su posterior uso con la librería Pandas para Python.

Por otro lado, el área de atención al cliente maneja un archivo CSV \textit{(comma-separated values)} con la jerarquía de casos L1, L2 y L3. Es decir, para cada L1, qué casos L2 le corresponden, y para cada uno de esos L2, qué casos L3 le corresponden. Las categorías L3 fueron ignoradas para este trabajo, por lo que se hará foco en las primeras dos.

En total, se encontraron 15 categorías L1 y 122 categorías L2. Luego de una reunión con el área de atención al cliente para un mejor entendimiento de cada una, se descartó una categoría especial. Luego de este refinamiento quedaron 14 L1 y 117 L2.

En la figura \ref{fig:catl2porl1} se puede observar para cada categoría L1 cuántas subcategorías L2 contiene.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{./Figures/catl2porl1.png}
	\caption{Cantidad de categorías L2 para cada L1.}
	\label{fig:catl2porl1}
\end{figure}

Para trabajar en la fase de preprocesamiento, se combinó el archivo de los casos con el archivo que contenía la jerarquía de categorías, obteniendo así la L1 de cada reclamo y consulta.

\subsection{\textit{Pipeline} de preprocesamiento}

El preprocesamiento es una de las tareas más importantes en PNL. Los datos en crudo pueden contener información que no es relevante para la tarea en cuestión, que se debe eliminar. Por otro lado, muchos modelos de IA requieren un formateo previo de los datos para poder consumirlos.

En la figura \ref{fig:pipeline-texto} se detallan los pasos realizados para ``limpiar'' el texto.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.7\textwidth]{./Figures/pipeline-texto.png}
	\caption{Diagrama del procesamiento del texto crudo.}
	\label{fig:pipeline-texto}
\end{figure}

El proceso comienza con el texto original del reclamo tal cual quedó guardado en la base. Luego se le aplican las siguientes acciones:
\begin{enumerate}
	\item Se suprimen los acentos de las palabras (si los hubiera) utilizando una expresión regular.
	\item Se eliminan los saltos de línea.
	\item Se sacan URLs en el mesaje en caso de que hubiese hipervínculos.
	\item Se sacan direcciones de \textit{email}. Estos por lo general aparecen cuando el reclamo se hace a través de ese medio.
	\item Se suprimen signos de puntuación utilizando otra expresión regular.
	\item Se eliminan espacios múltiples para normalizar en un espacio entre dos palabras.
	\item Se remueven frases que se repiten en muchos reclamos: algunos casos quedan registrados en la base con la respuesta por parte de la empresa. Estos mensajes tienen una plantilla de base con frases que luego se repiten entre casos.
	\item Se unen palabras con un significado específico: por ejemplo ``pedidos ya'' se deja como ``pedidosya'' porque hace referencia a una empresa.
	\item Para el caso de los modelos de CNB entrenados, se aplicó un paso extra para eliminar \textit{stopwords}.
	\item Por último se realizó un análisis con los datos de entrenamiento para ver los errores gramaticales más comunes utilizando la librería PyEnchant para Python. De esto se obtuvo un listado con los errores más comunes y un mapeo a su versión correcta. En este paso se aplica la corrección sobre ese listado de palabras.
\end{enumerate}

Una vez obtenido el texto limpio, se procede a pasarlo a un vector numérico para que sea intepretado por los modelos de IA. Se distinguen dos formas de hacerlo, dependiendo de si se usa el modelo de CNB o el de BERT.

\subsubsection{Codificación para CNB}

En la figura \ref{fig:pipeline-baseline} se explica el proceso para obtener los vectores numéricos para entrenar el modelo de CNB. Al texto limpio se le aplica un paso de segmentación para obtener los \textit{tokens} que luego serán vectorizados utilizando el método TF-IDF. Por otro lado, a la variable objetivo que también esta en formato de texto, se le aplica el proceso de \textit{label encoding} para obtener una variable numérica. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.7\textwidth]{./Figures/pipeline-baseline.png}
	\caption{Diagrama de codificación para CNB.}
	\label{fig:pipeline-baseline}
\end{figure}

\subsubsection{Codificación para BERT}

En la figura \ref{fig:pipeline-bert} se puede ver el proceso para el caso de BERT. Se puede notar que es un poco mas largo que el de CNB.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.7\textwidth]{./Figures/pipeline-bert.png}
	\caption{Diagrama de codificación para BERT.}
	\label{fig:pipeline-bert}
\end{figure}

El algoritmo de segmentación de BERT se llama \textit{WordPiece} y tiene la particularidad de que no sólo segmenta por palabras sino que también lo hace por subpalabras y por caracteres. Arranca segmentando por todos los caracteres del vocabulario con el que fue entrenado y luego los va uniendo en pares. 

Utiliza la siguiente fórmula para computar los puntajes de los pares de \textit{tokens} a unir:
\begin{equation}
puntaje = \frac{\text{frecuencia del par}}{\text{frecuencia del primer elemento}\times \text{frecuencia del segundo elemento}}
\end{equation}
Luego une los pares con mayor puntaje y re-calcula para todos los \textit{tokens} resultantes. Este proceso es repetido hasta alcanzar un umbral para el puntaje o hasta alcanzar un límite de \textit{tokens}.

El vocabulario del modelo pre-entrenado de BERT que se utilizó está compuesto por 977 \textit{tokens} reservados del estilo ``[MASK]'' o ``[PAD]'' por ejemplo y luego por \textit{tokens} de caracteres individuales, subpalabras y palabras. \citep{WEBSITE:22}

En la figura \ref{fig:segmentacion-bert} podemos ver un ejemplo de cómo es segmentada una oración.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{./Figures/cap3-segmentacion.png}
	\caption{Ejemplo de segmentación \textit{WordPiece}\protect\footnotemark.}
	\label{fig:segmentacion-bert}
\end{figure}

\footnotetext{Imagen tomada de \url{https://espejel.substack.com/p/beto-bert-01-importacion-y-tokenizing}}

Luego de hacer la segmentación de cada frase, se hace un truncamiento de las secuencias largas a un máximo preestablecido y se realiza un relleno a las secuencias cortas con un \textit{token} especial hasta ocupar ese mismo máximo.

Los siguientes pasos son obtener los \textit{input ids} que son los identificadores de los \textit{tokens}, obtener la máscara de atención que marca qué \textit{tokens} la red debe mirar y por último obtener los \textit{token type ids} que marcan a qué secuencia pertenece cada \textit{token} (recordar que BERT se entrena de a pares de secuencia).

Por otro lado, la codificación de la variable objetivo es igual que en CNB solo que a la variable numérica obtenida se le aplica una codificación \textit{one hot} adicional.

\section{Entrenamiento de los modelos}

En esta sección se describe en detalle: los análisis exploratorios realizados sobre los datos, cómo fue concebida la solución de los modelos de IA, y las técnicas utilizadas para entrenar los modelos.

\subsection{Análisis exploratorio}

En la figura \ref{fig:cap3-distribucion} se observa la proporción de casos de cada categoría L1 sobre el total. Se puede ver que hay una que tiene casi el 50\%.

\begin{figure}[htb]
	\centering
	\includegraphics[width=1\textwidth]{./Figures/cap3-distribucion.png}
	\caption{Proporción de casos.}
	\label{fig:cap3-distribucion}
\end{figure}

También se realizó un análisis de las longitudes de los textos. En promedio, la cantidad de palabras en los mensajes fue de 78 previo a aplicar preprocesamiento, 71 aplicando una parte (se dejan las \textit{stopwords}) y 44 cuando se aplicó el preprocesamiento completo.

En la figura \ref{fig:cap3-longitudes} podemos observar la longitud promedio para cada categoría. En cada barra se puede ver la longitud del texto sin preprocesamiento, con preprocesamiento dejando \textit{stopwords} y también removiéndolas. Hay dos categorías cuya longitud promedio es particularmente elevada.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{./Figures/cap3-longitudes.png}
	\caption{Longitud de los mensajes.}
	\label{fig:cap3-longitudes}
\end{figure}

\subsection{Integración de los modelos}

A continuación se presenta la figura \ref{fig:cap3-redes} dónde se puede ver cómo fue concebida la arquitectura de la solución de los modelos de IA.

Se tiene un clasificador que en primera instancia hace la predicción de las categorías de primer nivel, y luego se tienen clasificadores para cada subcategoría de segundo nivel. Esto significa que en total se entrenaron 15 modelos: el general y 14 de subcategorías.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.8\textwidth]{./Figures/cap3-redes.png}
	\caption{Diagrama general de los modelos de IA.}
	\label{fig:cap3-redes}
\end{figure}

Este diseño presenta una serie de ventajas, entre las que principalmente se destaca su diseño modular. Esto es importante para la etapa de soporte a futuro, ya que si surge una nueva categoría o se quiere mejorar una parte de la clasificación, sólo es necesario trabajar en el modelo afectado dejando los demás intactos.

Además, al hacer que cada clasificador tenga que realizar una tarea más específica, su \textit{head} de clasificación también queda más simple.

\subsection{Entrenamiento de los modelos base}

Un modelo base o \textit{baseline} es un modelo simple que actúa como referencia y sirve para tener con qué comparar cuando se entrenan modelos más complejos. 

Las ventajas de utilizar un modelo base son las siguientes \citep{WEBSITE:23}:
\begin{itemize}
	\item Se entrenan rápidamente y sin recursos especializados.
	\item Permite tener un mejor entendimiento en etapas tempranas.
	\item Sirven para encontrar errores y poner a prueba suposiciones.
	\item Sirven para medir el progreso de los modelos más complejos.
\end{itemize}

Como se mencionó en el capítulo \ref{Chapter2}, para este trabajo el \textit{baseline} elegido fue CNB de la librería Scikit-Learn. La clase provista se llama \textit{ComplementNB} y provee un método \textit{fit} para realizar el entrenamiento y un método \textit{predict} para realizar una predicción sobre datos que se pasan como parámetro.

El entorno donde se realizó el entrenamiento fue una computadora local.

\subsubsection{Modelos base en validación cruzada}

Hubo algunas categorías en particular que cuando se quiso entrenar el clasificador L2, contaban con muy pocos datos incluso tomando del año 2023. Estas categorías fueron:
\begin{itemize}
	\item L2\_L.
	\item L2\_N.
\end{itemize}

La primera contaba con un total de 200 casos y cuando se incluyeron los de 2023  se llegó a tener 267. Para este caso, lo que se hizo fue obtener una métrica en validación cruzada usando la clase RepeatedStratifiedKFold de Scikit-Learn con 5 \textit{folds} y 5 repeats. Luego se procedió a entrenar el modelo CNB final utilizando el set de datos completo.

La segunda contaba originalmente con sólo 2 casos y cuando se incluyeron los de 2023 se llegó a 8. Este resultó ser un caso muy extremo, donde se utilizó otra técnica de validación cruzada con la clase LeaveOneOut de Scikit-Learn. Lo que hace esta técnica es ir apartando un dato por \textit{fold} y entrenar con el resto. Una vez tomada la métrica, se entrenó el CNB final con todos los datos.

\subsection{Entrenamiento de BERT}

Como los textos del \textit{dataset} estaban escritos en español, se utilizó la red BETO. Este modelo es un modelo BERT entrenado con el dataset \textit{spanish unannotated corpora} que cuenta con casi tres millardos de palabras. Está disponible dentro de la librería Transformers de HuggingFace para Python, tanto en Pytorch como en Tensorflow. Para este trabajo se utilizó esta última.

Por otro lado, estos modelos pre-entrenados deben ser adaptados para realizar tareas como clasificación de texto. En la figura \ref{fig:cap3-downstream} se muestra un esquema de entrenamiento de BETO.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.9\textwidth]{./Figures/cap3-downstream.png}
	\caption{Esquema de entrenamiento de BERT para clasificación de texto.}
	\label{fig:cap3-downstream}
\end{figure}

Se puede notar que sobre el modelo pre-entrenado de BETO se agrega: un \textit{head} de clasificación compuesto por perceptrones multicapa combinados con técnicas de regularización como \textit{dropout} y por último \textit{softmax} como función de activación para realizar la tarea de clasificación per se.

\subsubsection{Aprendizaje por transferencia}

El aprendizaje por transferencia consiste en entrenar un modelo en un conjunto de datos a gran escala y luego usar ese modelo previamente entrenado para llevar a cabo el aprendizaje para otra tarea posterior (la tarea objetivo) \citep{WEBSITE:24}.

Se distinguen tres métodos para transferir el aprendizaje:
\begin{itemize}
	\item Extracción de características: se usan las capas pre-entrenadas para extraer características o \textit{features} de los datos. Los pesos de las capas inferiores no se actualizan durante el \textit{backpropagation}.
	\item Ajuste fino: se ajustan todos los parámetros del modelo.
	\item Extracción de capas: se extraen sólo las capas necesarias para la tarea en cuestión.
\end{itemize}

Para este trabajo, el aprendizaje por transferencia consistió en utilizar el modelo pre-entrenado de BETO y luego ajustarlo para la tarea de clasificación de reclamos.

En primera instancia se utilizó el método de extracción de características que consistió en congelar las capas de BETO y dejar libre el \textit{head} de clasificación para entrenar sus pesos. Para esto, se dejó el \textit{learning rate} default de Tensorflow.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.4\textwidth]{./Figures/cap3-feature-extraction.png}
	\caption{Técnica de extracción de características.}
	\label{fig:cap3-feature-extraction}
\end{figure}

Luego, se realizó un ajuste fino, dejando libres todas las capas del modelo para ajustar todos sus pesos, esta vez con un \textit{learning rate} más bajo.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.4\textwidth]{./Figures/cap3-fine-tuning.png}
	\caption{Técnica de ajuste fino.}
	\label{fig:cap3-fine-tuning}
\end{figure}

La técnica de extracción de capas no fue utilizada en este trabajo.

\subsubsection{Entorno de entrenamiento}

Como se menciona en el capítulo \ref{Chapter2}, para entrenar a BETO se utilizó un entorno de trabajo en la nube llamado Vertex AI Workbench. Se diferencian dos tipos de instancias:
\begin{itemize}
	\item Los \textit{notebooks} administrados: son entornos administrados por Google.
	\item Los \textit{notebooks} administrados por el usuario: permiten una mayor personalización y son ideales cuando se necesita mayor control sobre el entorno.
\end{itemize}

La instancia utilizada fue la segunda: las administradas por el usuario. El \textit{hardware} con el que contaba era el siguiente:
\begin{table}[h]
	\centering
	\caption[Especificaciones técnicas Vertex]{Especificaciones técnicas del entorno de entrenamiento.}
	\begin{tabular}{l c}    
		\toprule
		\textbf{Componente}			& 			\textbf{Descripción}  \\
		\midrule	
		Versión del ambiente		& 			M107  \\
		Tipo de máquina 			& 			n1-standard-8  \\
		Versión de CPU				&			Intel Xeon E5 \\
		Nro. de núcleos virtuales 	& 			8 vCPUs \\
		Memoria RAM 				& 			30 GB  \\
		Nro. de placas de video 	& 			1  \\
		Versión de placa de video 	& 			NVIDIA T4 \\
		Memoria de placa de video	& 			16 GB GDDR6  \\
		Almacenamiento				&			1 HDD x 200 GB \\
		\bottomrule
		\hline
	\end{tabular}
	\label{tab:vertex}
\end{table}

Por otro lado, se decidió utilizar el entorno TensorFlow Enterprise 2.11 como imagen virtual de la instancia, lo que permitió tener todos los \textit{drivers} de la placa de video pre-instalados y además contaba con el sistema de paquetes Conda con las librerías de TensorFlow para Python también pre-instaladas. Tener esa configuración cubierta por la nube de Google resultó en una gran ventaja porque evitó el tener que analizar la compatibilidad entre el \textit{hardware} y el \textit{software}.

\subsubsection{Puntos de control}

Para evitar pérdida de información accidental en la etapa de entrenamiento se utilizó una clase de TensorFlow que se llama ModelCheckpoint.

ModelCheckpoint es un \textit{callback} que, dependiendo de como se configure, puede ejecutarse al finalizar una época o cada cierta cantidad de lotes o \textit{batches}. Su función principal es guardar los pesos de la red y el estado del optimizador al almacenamiento permanente para no perder esa información en caso de que se apague la instancia o surja algún error. Se pueden configurar criterios de guardado como, por ejemplo, sólo si mejoró una métrica o la función de pérdida.

La configuración usada fue:
\begin{table}[h]
	\centering
	\caption[Configuración ModelCheckpoint]{Configuración del ModelCheckpoint.}
	\begin{tabular}{l c}    
		\toprule
		\textbf{Parámetro}			& 			\textbf{Valor}  \\
		\midrule	
		Nombre del archivo			& 			Nombre del archivo en extensión h5  \\
		Métrica a monitorear 		& 			Pérdida para lote de entrenamiento  \\
		Guardar sólo el mejor		&			Sí \\
		Modo de la métrica 			& 			Minimizar \\
		Frecuencia de guardado 		& 			En cada época  \\
		\bottomrule
		\hline
	\end{tabular}
	\label{tab:checkpoint}
\end{table}

\subsubsection{Parada temprana}

Otro de los mecanismos de prevención de sobreajuste o \textit{overfitting} utilizados fue la parada temprana o \textit{early stopping}. La idea detrás de esta técnica es detener el entrenamiento cuando el modelo empieza a disminuir su capacidad de generalización.

Esto se puede notar en la imagen \ref{fig:cap3-overfitting} donde la función de pérdida para el lote de entrenamiento continúa disminuyendo pero para el lote de validación no.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.8\textwidth]{./Figures/cap3-overfitting.png}
	\caption{Funciones de pérdida a través de las épocas.}
	\label{fig:cap3-overfitting}
\end{figure}

La clase de TensorFlow utilizada para esto fue EarlyStopping. También es un \textit{callback} pero este lo que hace es ir monitoreando una métrica que se pasa como parámetro y luego de un par de épocas en las que no mejora o empeora, detiene el entrenamiento.

La configuración utilizada fue la siguiente:
\begin{table}[h]
	\centering
	\caption[Configuración EarlyStopping]{Configuración de EarlyStopping.}
	\begin{tabular}{l c}    
		\toprule
		\textbf{Parámetro}			& 			\textbf{Valor}  \\
		\midrule	
		Paciencia					& 			5 épocas  \\
		Restaurar mejores pesos 	& 			Sí  \\
		\bottomrule
		\hline
	\end{tabular}
	\label{tab:earlystopping}
\end{table}

\subsubsection{Efecto de usar \textit{stopwords} en el entrenamiento}



\subsubsection{Técnicas para el desbalanceo de clases}



\subsubsection{Optimizadores}



\section{Desarrollo de GitHub Actions}



\section{Empaquetamiento del código y artefactos}



\section{Desarrollo del \textit{pipeline} de predicción}



\section{Almacenamiento de predicciones}

